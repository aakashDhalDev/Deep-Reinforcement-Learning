{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Setup\n"
      ],
      "metadata": {
        "id": "vm88EP_uC6tO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRWtR-dq1Ruz"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "id": "L3kml4rO2H0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ],
      "metadata": {
        "id": "JzlxKoLL-Jdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "rZV_wsdRaoMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "# from gym.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "gV7dftlZDYcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/MsPacman-v5', render_mode=None)\n",
        "# env = RecordVideo(env, video_folder='./video', episode_trigger=lambda x: True)\n",
        "\n",
        "observation = env.reset()"
      ],
      "metadata": {
        "id": "9er-WGJe1VEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_fn(current_reward, life_current, life_next, obs_current, obs_next):\n",
        "    extra_reward = 0\n",
        "    if life_current < life_next:\n",
        "      extra_reward -= 10\n",
        "    if (obs_current[0] == obs_next[0]).all():\n",
        "      extra_reward -= 1\n",
        "    if current_reward == 10:\n",
        "      extra_reward += 10\n",
        "    return extra_reward"
      ],
      "metadata": {
        "id": "f3WVSWng3J-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_preprocess(frame):\n",
        "  grayscale_img = np.mean(frame, axis=2)\n",
        "  resize_img = cv2.resize(grayscale_img, (144, 144), interpolation=cv2.INTER_AREA)\n",
        "  return resize_img.astype(np.float32)"
      ],
      "metadata": {
        "id": "FSNzZpoLMmhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = set()\n",
        "steps = 0\n",
        "life_current = 3\n",
        "reward = 0\n",
        "rewards = []\n",
        "done = False\n",
        "observation_current = img_preprocess(env.reset()[0])\n",
        "for _ in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    actions.add(action)\n",
        "    print(observation_current.shape)\n",
        "    print(type(observation_current))\n",
        "    observation_next, current_reward, done, info, check = env.step(action)\n",
        "    life_next = check[\"lives\"]\n",
        "    reward += (current_reward + reward_fn(current_reward, life_current, life_next, observation_current, img_preprocess(observation_next)))\n",
        "    # print(info)\n",
        "    if(current_reward==10):\n",
        "      print(\"10 reward!!!\")\n",
        "    print(\"cummulative reward\", reward)\n",
        "    observation_current = img_preprocess(observation_next)\n",
        "    steps+=1\n",
        "    life_current = life_next\n",
        "    if done and check[\"lives\"]==0:\n",
        "        observation = env.reset()\n",
        "        break\n",
        "env.close()\n",
        "print(steps)"
      ],
      "metadata": {
        "id": "bKQn4mq451Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions"
      ],
      "metadata": {
        "id": "4HhSXFH7o06s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation[0].shape"
      ],
      "metadata": {
        "id": "x5TxoKjOjNYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "R1mCg9xem-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Replay Buffer\n"
      ],
      "metadata": {
        "id": "_xEfsjG9QHl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "from typing import Tuple\n",
        "from numpy.random import binomial\n",
        "from numpy.random import choice\n",
        "import numpy.random as nr\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "Tensor = torch.DoubleTensor\n",
        "torch.set_default_tensor_type(Tensor)\n",
        "Transitions = namedtuple('Transitions', ['obs', 'action', 'reward', 'next_obs', 'done'])"
      ],
      "metadata": {
        "id": "JLdtZUZFQQbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, state_shape, action_shape, device='cpu'):\n",
        "        self.capacity = capacity\n",
        "        self.device = device\n",
        "        self.position = 0\n",
        "        self.buffer = {\n",
        "            'states': torch.empty((capacity, *state_shape), dtype=torch.float32, device=device),\n",
        "            'actions': torch.empty((capacity, action_shape), dtype=torch.long, device=device),\n",
        "            'rewards': torch.empty((capacity, 1), dtype=torch.float32, device=device),\n",
        "            'next_states': torch.empty((capacity, *state_shape), dtype=torch.float32, device=device),\n",
        "            'dones': torch.empty((capacity, 1), dtype=torch.float32, device=device)\n",
        "        }\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer['states'][self.position] = torch.tensor(state, device=self.device)\n",
        "        self.buffer['actions'][self.position] = torch.tensor(action, device=self.device)\n",
        "        self.buffer['rewards'][self.position] = torch.tensor([reward], device=self.device)\n",
        "        self.buffer['next_states'][self.position] = torch.tensor(next_state, device=self.device)\n",
        "        self.buffer['dones'][self.position] = torch.tensor([done], device=self.device)\n",
        "\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        if self.size < self.capacity:\n",
        "            self.size += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = torch.randint(0, self.size, (batch_size,), device=self.device)\n",
        "        batch = {k: v[indices] for k, v in self.buffer.items()}\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "OhOip8RNQKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_capacity = 1000\n",
        "batch_size = 32\n",
        "state_shape = (144, 144)\n",
        "action_shape = 9\n",
        "\n",
        "replay_buffer = ReplayBuffer(capacity=buffer_capacity, state_shape=state_shape, action_shape=action_shape, device=device)"
      ],
      "metadata": {
        "id": "vJZAwNMVzYt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN"
      ],
      "metadata": {
        "id": "ePTKkhW4DAJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, dim_action):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n",
        "        self.fc2 = nn.Linear(512, dim_action)\n",
        "\n",
        "        # self.apply(self._initialize_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = x.unsqueeze(0)\n",
        "        x = x.view(x.shape[1], x.shape[0], x.shape[2], x.shape[3])\n",
        "        # print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 18 * 18)  # Flatten layer\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "          if m.bias is not None:\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "          nn.init.constant_(m.weight, 1)\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "          nn.init.xavier_normal_(m.weight)\n",
        "          nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "CP_NLoxWEowe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = QNetwork(dim_action=9)"
      ],
      "metadata": {
        "id": "uZLDM0k-1uiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((144, 144))\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "psApM6G27HJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  val = Q.forward(x.unsqueeze(0))\n",
        "print(val)"
      ],
      "metadata": {
        "id": "GsiB3-xW7jbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, config):\n",
        "\n",
        "        torch.manual_seed(config['seed'])\n",
        "\n",
        "        self.lr = config['lr']  # learning rate\n",
        "        self.C = config['C']  # copy steps\n",
        "        self.eps_len = config['eps_len']  # length of epsilon greedy exploration\n",
        "        self.eps_max = config['eps_max']\n",
        "        self.eps_min = config['eps_min']\n",
        "        self.discount = config['discount']  # discount factor\n",
        "        self.batch_size = config['batch_size']  # mini batch size\n",
        "\n",
        "        self.dim_action = config['dim_action']\n",
        "\n",
        "        self.device = config['device']\n",
        "\n",
        "        self.Q = QNetwork(dim_action=self.dim_action)\n",
        "        self.Q_tar = QNetwork(dim_action=self.dim_action)\n",
        "\n",
        "        self.Q = self.Q.to(self.device)\n",
        "        self.Q_tar = self.Q_tar.to(self.device)\n",
        "\n",
        "        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr)\n",
        "        self.training_step = 0\n",
        "\n",
        "    def update(self, buffer):\n",
        "        t = buffer.sample(self.batch_size)\n",
        "\n",
        "        s = t[\"states\"].to(self.device)\n",
        "        a = t[\"actions\"].to(self.device)\n",
        "        r = t[\"rewards\"].to(self.device)\n",
        "        sp = t[\"next_states\"].to(self.device)\n",
        "        done = t[\"dones\"].to(self.device)\n",
        "\n",
        "        self.training_step += 1\n",
        "        current_q_values = self.Q.forward(s.double()).gather(1, a)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            expected_q_values = r + self.discount * torch.max(self.Q_tar.forward(sp.double()), axis=1, keepdim=True)[0] * (1-done)\n",
        "\n",
        "        # print(\"current:\", current_q_values.shape)\n",
        "        # print(current_q_values)\n",
        "        # print(\"expected:\", expected_q_values.shape)\n",
        "        # print(expected_q_values)\n",
        "        loss = nn.functional.mse_loss(current_q_values.mean(dim=1, keepdim=True),expected_q_values)\n",
        "\n",
        "        self.optimizer_Q.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer_Q.step()\n",
        "\n",
        "        if self.training_step % self.C == 0:\n",
        "            self.Q_tar.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    def act_probabilistic(self, observation: torch.Tensor):\n",
        "        # epsilon greedy:\n",
        "        # print(\"Obs shape: \", observation.shape)\n",
        "        first_term = self.eps_max * (self.eps_len - self.training_step) / self.eps_len\n",
        "        eps = max(first_term, self.eps_min)\n",
        "\n",
        "        explore = binomial(1, eps)\n",
        "\n",
        "        if explore == 1:\n",
        "            a = choice(self.dim_action)\n",
        "        else:\n",
        "            self.Q.eval()\n",
        "            with torch.no_grad():\n",
        "              Q = self.Q(observation.unsqueeze(0).double())\n",
        "            val, a = torch.max(Q, axis=1)\n",
        "            a = a.item()\n",
        "        return a\n"
      ],
      "metadata": {
        "id": "qpGA0GmGQfCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dim_action': 9,  # Q network output\n",
        "    'lr': 0.0005,  # learning rate\n",
        "    'C': 60,  # copy steps\n",
        "    'discount': 0.99,  # discount factor\n",
        "    'batch_size': 32,\n",
        "    'replay_buffer_size': 100000,\n",
        "    'eps_min': 0.01,\n",
        "    'eps_max': 1.0,\n",
        "    'eps_len': 4000,\n",
        "    'seed': 1,\n",
        "    'device':device\n",
        "}"
      ],
      "metadata": {
        "id": "PPmcU6YMQtoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "YtQLKBMjdo_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN(config)\n",
        "train_writer = SummaryWriter(log_dir='tensorboard/dqn')"
      ],
      "metadata": {
        "id": "jONzzFBtadbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0  # total number of steps\n",
        "rewards = []\n",
        "for i_episode in range(500):\n",
        "    observation = env.reset()[0]\n",
        "    done = 0\n",
        "    t = 0  # time steps within each episode\n",
        "    ret = 0.  # episodic return\n",
        "    life_current = 3\n",
        "    for _ in range(1000):\n",
        "        observation_tensor = torch.tensor(img_preprocess(observation)).to(device)\n",
        "        action = dqn.act_probabilistic(observation_tensor)  # take action\n",
        "\n",
        "        observation_next, current_reward, done, info, check = env.step(action)\n",
        "        life_next = check[\"lives\"]\n",
        "\n",
        "        reward = current_reward + reward_fn(current_reward, life_current, life_next, img_preprocess(observation), img_preprocess(observation_next))\n",
        "        # print(reward)\n",
        "        replay_buffer.add(state=observation_tensor,  # put the transition to memory\n",
        "                             action=torch.from_numpy(np.array([action])),\n",
        "                             reward=torch.from_numpy(np.array([reward])),\n",
        "                             next_state=torch.from_numpy(img_preprocess(observation_next)),\n",
        "                             done=done)\n",
        "\n",
        "        dqn.update(replay_buffer)  # agent learn\n",
        "\n",
        "        t += 1\n",
        "        steps += 1\n",
        "        observation = observation_next\n",
        "        ret += reward\n",
        "        life_current = life_next\n",
        "        if done and life_current==0:\n",
        "            print(\"Episode {} finished after {} timesteps\".format(i_episode, t+1))\n",
        "            rewards.append(ret)\n",
        "            break\n",
        "\n",
        "        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)  # plot\n",
        "\n",
        "    print(f\"Return in episode {i_episode} : {ret}\")\n",
        "\n",
        "env.close()\n",
        "train_writer.close()"
      ],
      "metadata": {
        "id": "YaG-YOP5ZGws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rewards)"
      ],
      "metadata": {
        "id": "E59DVbd-sleg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='tensorboard/dqn'"
      ],
      "metadata": {
        "id": "-xP8qGdP1GY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DDPG"
      ],
      "metadata": {
        "id": "MEizHJA01tJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, dim_action):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n",
        "        self.fc2 = nn.Linear(512, dim_action)\n",
        "\n",
        "        self.apply(self._initialize_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = x.unsqueeze(0).double()\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.shape[1], x.shape[0], x.shape[2], x.shape[3])\n",
        "        # print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 18 * 18)  # Flatten layer\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.argmax(x, dim=-1)\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "          if m.bias is not None:\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "          nn.init.constant_(m.weight, 1)\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "          nn.init.xavier_normal_(m.weight)\n",
        "          nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "4KLqGt5NiPkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QCriticNet(nn.Module):\n",
        "    def __init__(self, dim_action):\n",
        "        super(QCriticNet, self).__init__()\n",
        "        self.dim_action = dim_action\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 18 * 18+dim_action, 512)\n",
        "        self.fc2 = nn.Linear(512, dim_action)\n",
        "\n",
        "        self.apply(self._initialize_weights)\n",
        "\n",
        "    def forward(self, x, action):\n",
        "        # print(x.shape)\n",
        "        x = x.unsqueeze(0).double()\n",
        "        x = x.view(x.shape[1], x.shape[0], x.shape[2], x.shape[3])\n",
        "        # print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 18 * 18)  # Flatten layer\n",
        "        # print(\"action shape in QNet(before mean conversion): \",action.shape)\n",
        "        # print(action)\n",
        "        if(len(action.shape)==2):\n",
        "          # print(\"2d action flaged!!!\")\n",
        "          action = action[:,0]\n",
        "        # print(\"action shape in QNet(after mean conversion): \",action.shape)\n",
        "        ohe_action_space = F.one_hot(action.to(int), num_classes=9).squeeze(1)\n",
        "        # print(\"ohe shape in QNet(after ohe conversion): \", ohe_action_space.shape)\n",
        "        # print(\"x shape in QNet: \", x.shape)\n",
        "        #   print(\"ohe shape in QNet(after ohe conversion then this internal transformation): \", ohe_action_space.shape)\n",
        "        # print(ohe_action_space)\n",
        "        x = torch.cat((x, ohe_action_space), dim=-1)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.argmax(x, dim=-1)\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "          if m.bias is not None:\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "          nn.init.constant_(m.weight, 1)\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "          nn.init.xavier_normal_(m.weight)\n",
        "          nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "cXPCmo5g1ryn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DDPG,self).__init__()\n",
        "        torch.manual_seed(config['seed'])\n",
        "\n",
        "        self.lr_actor = config['lr_actor']  # learning rate\n",
        "        self.lr_critic = config['lr_critic']\n",
        "        self.smooth = config['smooth']  # smoothing coefficient for target net\n",
        "        self.discount = config['discount']  # discount factor\n",
        "        self.batch_size = config['batch_size']  # mini batch size\n",
        "        self.sig = config['sig']  # exploration noise\n",
        "\n",
        "        self.dim_action = config['dim_action']\n",
        "\n",
        "        self.device = config['device']\n",
        "\n",
        "        self.actor = ActorNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.Q = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.actor_tar = ActorNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.Q_tar = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "\n",
        "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
        "        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr_critic)\n",
        "\n",
        "    def update(self, buffer):\n",
        "        t = buffer.sample(self.batch_size)\n",
        "\n",
        "        obs = t[\"states\"].to(self.device)\n",
        "        action = t[\"actions\"].to(self.device)\n",
        "        reward = t[\"rewards\"].to(self.device)\n",
        "        next_obs = t[\"next_states\"].to(self.device)\n",
        "        done = t[\"dones\"].to(self.device)\n",
        "\n",
        "        # print(\"area 1 start\")\n",
        "        with torch.no_grad():\n",
        "          y = reward + self.discount*(1-done)*self.Q_tar(next_obs, self.actor_tar(next_obs))\n",
        "\n",
        "\n",
        "        # print(\"area 2 start\")\n",
        "        loss_Q = torch.mean((self.Q(obs,action)-y)**2)\n",
        "        loss_Q.requires_grad = True\n",
        "        # print(\"area 2 end\")\n",
        "        self.optimizer_Q.zero_grad()\n",
        "        loss_Q.backward()\n",
        "        self.optimizer_Q.step()\n",
        "        # print(\"area 3 start\")\n",
        "        objective_actor = torch.mean(self.Q(obs,self.actor(obs).double()))\n",
        "        # print(\"area 3 end\")\n",
        "        loss_actor = -objective_actor\n",
        "\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        for target_param, param in zip(self.Q_tar.parameters(), self.Q.parameters()):\n",
        "          target_param.data.copy_(target_param.data*(self.smooth)+param.data*(1-self.smooth))\n",
        "\n",
        "        for target_param, param in zip(self.actor_tar.parameters(), self.actor.parameters()):\n",
        "          target_param.data.copy_(target_param.data*(self.smooth)+param.data*(1-self.smooth))\n",
        "\n",
        "    def act_probabilistic(self, obs: torch.Tensor):\n",
        "        self.actor.eval()\n",
        "        exploration_noise = torch.normal(torch.zeros(size=(self.dim_action,)), self.sig).to(self.device)\n",
        "        a = self.actor(obs.unsqueeze(0)) + exploration_noise\n",
        "        self.actor.train()\n",
        "        return torch.argmax(a, dim=-1)\n",
        "\n",
        "    def act_deterministic(self, obs: torch.Tensor):\n",
        "        self.actor.eval()\n",
        "        a = self.actor(obs)\n",
        "        self.actor.train()\n",
        "        return a"
      ],
      "metadata": {
        "id": "uarwN1LdUabj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dim_action': 9,\n",
        "    'lr_actor': 0.001,\n",
        "    'lr_critic': 0.005,\n",
        "    'smooth': 0.99,\n",
        "    'discount': 0.99,\n",
        "    'sig': 0.01,\n",
        "    'batch_size': 32,\n",
        "    'replay_buffer_size': 20000,\n",
        "    'seed': 1,\n",
        "    'max_episode': 500,\n",
        "    'device':device\n",
        "}"
      ],
      "metadata": {
        "id": "lLs1fckeUfaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddpg = DDPG(config)\n",
        "train_writer = SummaryWriter(log_dir='tensorboard/ddpg')"
      ],
      "metadata": {
        "id": "hYhpt0XVUkKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0  # total number of steps\n",
        "rewards = []\n",
        "for i_episode in range(300):\n",
        "    observation = env.reset()[0]\n",
        "    done = 0\n",
        "    t = 0  # time steps within each episode\n",
        "    ret = 0.  # episodic return\n",
        "    life_current = 3\n",
        "    for _ in range(1000):\n",
        "        observation_tensor = torch.tensor(img_preprocess(observation)).to(device)\n",
        "        action = ddpg.act_probabilistic(observation_tensor).cpu().item()  # take action\n",
        "\n",
        "        observation_next, current_reward, done, info, check = env.step(action)\n",
        "        life_next = check[\"lives\"]\n",
        "\n",
        "        reward = current_reward + reward_fn(current_reward, life_current, life_next, img_preprocess(observation), img_preprocess(observation_next))\n",
        "        # print(reward)\n",
        "        replay_buffer.add(state=observation_tensor,  # put the transition to memory\n",
        "                             action=torch.from_numpy(np.array([action])),\n",
        "                             reward=torch.from_numpy(np.array([reward])),\n",
        "                             next_state=torch.from_numpy(img_preprocess(observation_next)),\n",
        "                             done=done)\n",
        "\n",
        "        ddpg.update(replay_buffer)  # agent learn\n",
        "\n",
        "        t += 1\n",
        "        steps += 1\n",
        "        observation = observation_next\n",
        "        ret += reward\n",
        "        life_current = life_next\n",
        "        if done and life_current==0:\n",
        "            print(\"Episode {} finished after {} timesteps\".format(i_episode, t+1))\n",
        "            rewards.append(ret)\n",
        "            break\n",
        "\n",
        "        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)  # plot\n",
        "\n",
        "    print(f\"Return in episode {i_episode} : {ret}\")\n",
        "\n",
        "env.close()\n",
        "train_writer.close()"
      ],
      "metadata": {
        "id": "w9Hbby6qUuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='tensorboard/ddpg'"
      ],
      "metadata": {
        "id": "e2BHkkOGU2o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAC"
      ],
      "metadata": {
        "id": "-FNlG6t211dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, dim_action: int, device):\n",
        "\n",
        "        super(ActorNet, self).__init__()\n",
        "\n",
        "        self.dim_action = dim_action\n",
        "        self.device = device\n",
        "\n",
        "        self.ln2pi = torch.tensor(np.log(2*np.pi)).to(self.device)\n",
        "\n",
        "        self.convLayer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=6, stride=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=3)\n",
        "        )\n",
        "        self.convLayer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 28, kernel_size=5, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(28*15*11, 512)\n",
        "\n",
        "        self.mu = nn.Linear(512, self.dim_action)\n",
        "        self.log_sig = nn.Linear(512, self.dim_action)\n",
        "\n",
        "        self.apply(self._initialize_weights)\n",
        "\n",
        "    def forward(self, observation: torch.Tensor):\n",
        "        x = observation.double()\n",
        "        # print(\"input ready\")\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.shape[0],x.shape[3],x.shape[1],x.shape[2])\n",
        "        # print(x.shape)\n",
        "        x = self.convLayer1(x)\n",
        "        # print(\"conv layer 1 op ready\")\n",
        "        x = self.convLayer2(x)\n",
        "        # print(\"conv op:\", x.shape)\n",
        "        x = x.reshape(-1, 28*15*11)\n",
        "        # print(\"conv op:\", x.shape)\n",
        "        x = F.relu(self.fc(x))\n",
        "        mu_dist = self.mu(x)\n",
        "        sig_dist = torch.exp(self.log_sig(x))\n",
        "        # print(x.shape)\n",
        "        u = mu_dist + sig_dist * torch.normal(torch.zeros(size=mu_dist.shape), 1).to(self.device)\n",
        "        a = torch.tanh(u)\n",
        "        logProbu = -1/2 * (torch.sum(torch.log(sig_dist**2), dim=1, keepdims=True).to(self.device) +\n",
        "                           torch.sum((u-mu_dist)**2/sig_dist**2, dim=1, keepdims=True) +\n",
        "                           a.shape[1]*self.ln2pi.to(self.device))\n",
        "        logProba = logProbu - torch.sum(torch.log(1 - a ** 2 + 0.000001), dim=1, keepdims=True)\n",
        "\n",
        "        return a, logProba, torch.tanh(mu_dist)\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "          nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "          if m.bias is not None:\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "          nn.init.constant_(m.weight, 1)\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "          nn.init.xavier_normal_(m.weight)\n",
        "          nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "D2nLCSHhItvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAC(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SAC,self).__init__()\n",
        "        torch.manual_seed(config['seed'])\n",
        "\n",
        "        self.lr = config['lr']  # learning rate\n",
        "        self.smooth = config['smooth']  # smoothing coefficient for target net\n",
        "        self.discount = config['discount']  # discount factor\n",
        "        self.alpha = config['alpha']  # temperature parameter in SAC\n",
        "        self.batch_size = config['batch_size']  # mini batch size\n",
        "\n",
        "        self.dim_action = config['dim_action']\n",
        "\n",
        "        self.device = config['device']\n",
        "\n",
        "        self.actor = ActorNet(dim_action=self.dim_action, device = self.device).to(self.device)\n",
        "        self.Q1 = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.Q2 = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.Q1_tar = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "        self.Q2_tar = QCriticNet(dim_action=self.dim_action).to(self.device)\n",
        "\n",
        "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        self.optimizer_Q1 = torch.optim.Adam(self.Q1.parameters(), lr=self.lr)\n",
        "        self.optimizer_Q2 = torch.optim.Adam(self.Q2.parameters(), lr=self.lr)\n",
        "\n",
        "    def update(self, buffer):\n",
        "        # sample from replay memory\n",
        "        t = buffer.sample(self.batch_size)\n",
        "\n",
        "        obs = t[\"states\"].to(self.device)\n",
        "        action = t[\"actions\"].to(self.device)\n",
        "        reward = t[\"rewards\"].to(self.device)\n",
        "        next_obs = t[\"next_states\"].to(self.device)\n",
        "        done = t[\"dones\"].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          next_action_sample, next_logProb_sample, next_mu_sample = self.actor(next_obs)\n",
        "          Qp = torch.min(self.Q1_tar(next_obs,next_action_sample), self.Q2_tar(next_obs,next_action_sample))\n",
        "          Q_target = reward + self.discount*(~done)*(Qp-self.alpha*next_logProb_sample)\n",
        "\n",
        "        loss_Q1 = torch.mean((self.Q1(obs,action)-Q_target)**2)\n",
        "        loss_Q2 = torch.mean((self.Q2(obs,action)-Q_target)**2)\n",
        "\n",
        "        self.optimizer_Q1.zero_grad()\n",
        "        loss_Q1.backward()\n",
        "        self.optimizer_Q1.step()\n",
        "\n",
        "        self.optimizer_Q2.zero_grad()\n",
        "        loss_Q2.backward()\n",
        "        self.optimizer_Q2.step()\n",
        "\n",
        "        action_sample, logProb_sample, mu_sample = self.actor(obs)\n",
        "        Q = torch.min(self.Q1(obs,action_sample), self.Q2(obs,action_sample))\n",
        "        objective_actor = torch.mean(Q-self.alpha*logProb_sample)\n",
        "        loss_actor = -objective_actor\n",
        "\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        for target_param, param in zip(self.Q1_tar.parameters(), self.Q1.parameters()):\n",
        "          target_param.data.copy_(target_param.data*(1-self.smooth)+param.data*self.smooth)\n",
        "\n",
        "        for target_param, param in zip(self.Q2_tar.parameters(), self.Q2.parameters()):\n",
        "          target_param.data.copy_(target_param.data*(1-self.smooth)+param.data*self.smooth)\n",
        "\n",
        "    def act_probabilistic(self, obs: torch.Tensor):\n",
        "        self.actor.eval()\n",
        "        a, logProb, mu = self.actor(obs)\n",
        "        self.actor.train()\n",
        "        return a\n",
        "\n",
        "    def act_deterministic(self, obs: torch.Tensor):\n",
        "        self.actor.eval()\n",
        "        a, logProb, mu = self.actor(obs)\n",
        "        self.actor.train()\n",
        "        return mu"
      ],
      "metadata": {
        "id": "fmdaBtvY12zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'dim_action': 2,\n",
        "    'lr': 0.001,\n",
        "    'smooth': 0.99,\n",
        "    'discount': 0.99,\n",
        "    'alpha': 0.2,\n",
        "    'batch_size': 128,\n",
        "    'replay_buffer_size': 20000,\n",
        "    'seed': 1,\n",
        "    'max_episode': 500,\n",
        "    'device':device\n",
        "}"
      ],
      "metadata": {
        "id": "PoN3bXsmEXxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sac = SAC(config)\n",
        "train_writer = SummaryWriter(log_dir='tensorboard/sac')"
      ],
      "metadata": {
        "id": "IWilt6ZpckPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, state_shape, action_dim, max_size=int(1e6)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        # Adjusted for 3D state tensors\n",
        "        self.state = torch.zeros((max_size, *state_shape))\n",
        "        self.action = torch.zeros(max_size, action_dim)  # Actions are probability distributions\n",
        "        self.next_state = torch.zeros((max_size, *state_shape))\n",
        "        self.reward = torch.zeros(max_size, 1)\n",
        "        self.not_done = torch.zeros(max_size, 1)\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = torch.from_numpy(state)\n",
        "        self.action[self.ptr] = torch.from_numpy(action)\n",
        "        self.next_state[self.ptr] = torch.from_numpy(next_state)\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = torch.randint(0, self.size, (batch_size,), dtype=torch.long)\n",
        "\n",
        "        return (\n",
        "            self.state[ind],\n",
        "            self.action[ind],\n",
        "            self.next_state[ind],\n",
        "            self.reward[ind],\n",
        "            self.not_done[ind]\n",
        "        )"
      ],
      "metadata": {
        "id": "PTIjfUGISyAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "for i_episode in range(config['max_episode']):\n",
        "    obs = env.reset()[0]\n",
        "    done = False\n",
        "    truncated = False\n",
        "    t = 0\n",
        "    ret = 0.\n",
        "    while done is False and truncated is False:\n",
        "        # env.render()\n",
        "\n",
        "        obs_tensor = torch.tensor(obs).type(Tensor).to(device)\n",
        "\n",
        "        action = sac.act_probabilistic(obs_tensor.unsqueeze(0)).detach().cpu().numpy()[0,:]\n",
        "        # print(action[0])\n",
        "\n",
        "        next_obs, reward, done, truncated,_ = env.step(int(action[0]))\n",
        "\n",
        "        replay_buffer.add(state=obs_tensor,\n",
        "                             action=torch.from_numpy(action),\n",
        "                             reward=torch.from_numpy(np.array([reward/10.0])),\n",
        "                             next_state=torch.from_numpy(next_obs).type(Tensor),\n",
        "                             done=done)\n",
        "\n",
        "        # sac.update(replay_buffer)\n",
        "\n",
        "        t += 1\n",
        "        steps += 1\n",
        "        ret += reward\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "        if done or truncated:\n",
        "            print(\"Episode {} return {}\".format(i_episode, ret))\n",
        "    train_writer.add_scalar('Performance/episodic_return', ret, i_episode)\n",
        "\n",
        "env.close()\n",
        "train_writer.close()"
      ],
      "metadata": {
        "id": "zx8LJOsTcZ_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='tensorboard/sac'"
      ],
      "metadata": {
        "id": "0TBLUYTLcgRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}